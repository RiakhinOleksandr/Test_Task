{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e08e9e",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from rasterio.windows import Window\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953560ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image CRS: EPSG:32636\n"
     ]
    }
   ],
   "source": [
    "# Reading geojson file\n",
    "df = gpd.read_file('full_dataset/deforestation_labels.geojson')\n",
    "df[\"img_date\"] = gpd.pd.to_datetime(df[\"img_date\"])\n",
    "\n",
    "path = r\"full_dataset\\S2A_MSIL1C_20160212T084052_N0201_R064_T36UYA_20160212T084510\\S2A_MSIL1C_20160212T084052_N0201_R064_T36UYA_20160212T084510.SAFE\\GRANULE\\L1C_T36UYA_A003350_20160212T084510\\IMG_DATA\\T36UYA_20160212T084052_B02.jp2\"\n",
    "\n",
    "# Setting the same coordinate system as in images in dataframe\n",
    "with rasterio.open(path) as src:\n",
    "    print(\"Image CRS:\", src.crs)\n",
    "    df = df.to_crs(src.crs)\n",
    "\n",
    "# All Multipolygons in dataframe are divided into multiple polygons\n",
    "df = df.explode(index_parts=False).reset_index(drop=True)\n",
    "\n",
    "# Setting rectangle bounds for easier use\n",
    "df[[\"minx\", \"miny\", \"maxx\", \"maxy\"]] = df.geometry.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f1e041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_date</th>\n",
       "      <th>tile</th>\n",
       "      <th>geometry</th>\n",
       "      <th>minx</th>\n",
       "      <th>miny</th>\n",
       "      <th>maxx</th>\n",
       "      <th>maxy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>36UXA</td>\n",
       "      <td>POLYGON ((699307.129 5561713.193, 699414.418 5...</td>\n",
       "      <td>699246.875642</td>\n",
       "      <td>5.561537e+06</td>\n",
       "      <td>699432.480177</td>\n",
       "      <td>5.561713e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>36UXA</td>\n",
       "      <td>POLYGON ((698548.1 5553743.058, 698658.019 555...</td>\n",
       "      <td>698548.100157</td>\n",
       "      <td>5.553503e+06</td>\n",
       "      <td>698686.443698</td>\n",
       "      <td>5.553799e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>36UXA</td>\n",
       "      <td>POLYGON ((699613.313 5543770.618, 699605.429 5...</td>\n",
       "      <td>699605.429207</td>\n",
       "      <td>5.543771e+06</td>\n",
       "      <td>699921.463862</td>\n",
       "      <td>5.543889e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>36UXA</td>\n",
       "      <td>POLYGON ((699203.094 5542952.982, 699333.117 5...</td>\n",
       "      <td>699202.900300</td>\n",
       "      <td>5.542791e+06</td>\n",
       "      <td>699333.116689</td>\n",
       "      <td>5.542958e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>36UXA</td>\n",
       "      <td>POLYGON ((700515.258 5541902.106, 700605.663 5...</td>\n",
       "      <td>700515.258347</td>\n",
       "      <td>5.541572e+06</td>\n",
       "      <td>700606.448293</td>\n",
       "      <td>5.541921e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    img_date   tile                                           geometry  \\\n",
       "0 2016-04-09  36UXA  POLYGON ((699307.129 5561713.193, 699414.418 5...   \n",
       "1 2016-04-09  36UXA  POLYGON ((698548.1 5553743.058, 698658.019 555...   \n",
       "2 2016-04-09  36UXA  POLYGON ((699613.313 5543770.618, 699605.429 5...   \n",
       "3 2016-04-09  36UXA  POLYGON ((699203.094 5542952.982, 699333.117 5...   \n",
       "4 2016-04-09  36UXA  POLYGON ((700515.258 5541902.106, 700605.663 5...   \n",
       "\n",
       "            minx          miny           maxx          maxy  \n",
       "0  699246.875642  5.561537e+06  699432.480177  5.561713e+06  \n",
       "1  698548.100157  5.553503e+06  698686.443698  5.553799e+06  \n",
       "2  699605.429207  5.543771e+06  699921.463862  5.543889e+06  \n",
       "3  699202.900300  5.542791e+06  699333.116689  5.542958e+06  \n",
       "4  700515.258347  5.541572e+06  700606.448293  5.541921e+06  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4de550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_band_files(base_dir, tile_id, date_str):\n",
    "    \"\"\"\n",
    "    Returns dict of {band_name: path} for given tile_id and date (YYYYMMDD).\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir)\n",
    "    band_paths = {}\n",
    "    for jp2 in base_path.rglob(f\"T{tile_id}_{date_str}*_B*.jp2\"):\n",
    "        # jp2.name like T36UYA_20160212T084052_B04.jp2\n",
    "        band_name = jp2.stem.split(\"_\")[-1]  # 'B04'\n",
    "        band_paths[band_name] = jp2\n",
    "    return band_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da397ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42 folders with date that are given in dataframe\n",
      "There are also 4 folders more with date that are given in dataframe\n",
      "Now we have 1483 rows in dataset\n"
     ]
    }
   ],
   "source": [
    "counterOne = 0\n",
    "counterTwo = 0\n",
    "cleaned_df = df.copy()\n",
    "\n",
    "# There are many rows in dataframe that are not attached to images in folders we have at all. We must clean dataframe\n",
    "for date in df[\"img_date\"].unique():\n",
    "    bands1 = find_band_files(\"full_dataset\", \"36UXA\", date.strftime(\"%Y%m%d\"))\n",
    "    bands2 = find_band_files(\"full_dataset\", \"36UYA\", date.strftime(\"%Y%m%d\"))\n",
    "    if bands1 and bands2:\n",
    "        counterTwo += 1\n",
    "    elif bands1 or bands2:\n",
    "        counterOne += 1\n",
    "    else:\n",
    "        cleaned_df = cleaned_df[cleaned_df[\"img_date\"] != date]\n",
    "\n",
    "print(f\"There are {counterOne} folders with date that are given in dataframe\")\n",
    "print(f\"There are also {2 * counterTwo} folders more with date that are given in dataframe\")\n",
    "print(f\"Now we have {cleaned_df.shape[0]} rows in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f62d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is helpfull for acquaintance with dataset, but it is not used in this .ipynb file\n",
    "def load_rgb_from_safe(base_dir, tile_id, date_str):\n",
    "    \"\"\"\n",
    "    For a given tile and date finds B02, B03 and B04 files and making rgb array using them.\n",
    "    \"\"\"\n",
    "    band_paths = find_band_files(base_dir, tile_id, date_str)\n",
    "    with rasterio.open(band_paths[\"B04\"]) as r:\n",
    "        red = r.read(1).astype(np.float32)\n",
    "        transform = r.transform \n",
    "    with rasterio.open(band_paths[\"B03\"]) as g:\n",
    "        green = g.read(1).astype(np.float32)\n",
    "    with rasterio.open(band_paths[\"B02\"]) as b:\n",
    "        blue = b.read(1).astype(np.float32)\n",
    "    rgb = np.stack([red, green, blue], axis=-1)\n",
    "    return rgb, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285fca1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            area_m2\n",
      "count   1483.000000\n",
      "mean    4159.749890\n",
      "std     5746.969112\n",
      "min        0.331636\n",
      "25%      747.152385\n",
      "50%     1893.633049\n",
      "75%     5557.505381\n",
      "max    51979.029961\n"
     ]
    }
   ],
   "source": [
    "# Calculating area of polygons\n",
    "cleaned_df[\"area_m2\"] = cleaned_df.geometry.area\n",
    "\n",
    "print(cleaned_df[[\"area_m2\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb04cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 1483 rows\n",
      "There are 1244 rows now\n",
      "            area_m2\n",
      "count   1244.000000\n",
      "mean    4898.574829\n",
      "std     5998.808731\n",
      "min      501.777945\n",
      "25%     1180.015200\n",
      "50%     2509.856928\n",
      "75%     6679.559261\n",
      "max    51979.029961\n"
     ]
    }
   ],
   "source": [
    "# Deletting all polygons that are too small\n",
    "print(f\"There were {cleaned_df.shape[0]} rows\")\n",
    "\n",
    "cleaned_df = cleaned_df[cleaned_df[\"area_m2\"] > 500]\n",
    "\n",
    "print(f\"There are {cleaned_df.shape[0]} rows now\")\n",
    "print(cleaned_df[[\"area_m2\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rgb_crop(base_dir, tile_id, date_str, minx, maxx, miny, maxy, size=32):\n",
    "    \"\"\"\n",
    "    Loads a 32x32 RGB crop (B04, B03, B02) from Sentinel-2 data.\n",
    "    The crop is centered in the given bounding box (map coordinates).\n",
    "    \"\"\"\n",
    "    # Find band file paths\n",
    "    bands = find_band_files(base_dir, tile_id, date_str)\n",
    "    \n",
    "    # Get center coordinates of bbox\n",
    "    center_x = (minx + maxx) / 2\n",
    "    center_y = (miny + maxy) / 2\n",
    "    \n",
    "    rgb_bands = []\n",
    "    \n",
    "    for band_name in [\"B04\", \"B03\", \"B02\"]:  # R, G, B\n",
    "        path = bands[band_name]\n",
    "        with rasterio.open(path) as src:\n",
    "            width, height = src.width, src.height\n",
    "            col, row = src.index(center_x, center_y)\n",
    "            half = size // 2\n",
    "            \n",
    "            # Compute window boundaries (clip to image size)\n",
    "            col_start = max(col - half, 0)\n",
    "            row_start = max(row - half, 0)\n",
    "            col_end = min(col + half, width)\n",
    "            row_end = min(row + half, height)\n",
    "            \n",
    "            if col_end < col_start:\n",
    "                col_start, col_end = col_end, col_start\n",
    "            if row_end < row_start:\n",
    "                row_start, row_end = row_end, row_start\n",
    "\n",
    "            w = col_end - col_start\n",
    "            h = row_end - row_start\n",
    "\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue\n",
    "\n",
    "            window = Window(col_start, row_start, w, h)\n",
    "            \n",
    "            # Read one band crop\n",
    "            patch = src.read(1, window=window)\n",
    "\n",
    "            valid_pixels = np.count_nonzero(patch)\n",
    "            if valid_pixels < 1000:\n",
    "                return None \n",
    "\n",
    "            # Pad if at edge\n",
    "            if patch.shape[0] < size or patch.shape[1] < size:\n",
    "                pad_h = size - patch.shape[0]\n",
    "                pad_w = size - patch.shape[1]\n",
    "                patch = np.pad(patch, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            \n",
    "            rgb_bands.append(patch)\n",
    "    \n",
    "    # Stack bands → (3, size, size)\n",
    "    rgb = np.stack(rgb_bands, axis=0).astype(np.float32)\n",
    "    \n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_percentile_normalize(rgb, lower=1, upper=99):\n",
    "    \"\"\"\n",
    "    Percentile normalization across each channel of an RGB patch.\n",
    "    Ignores NaNs and zeros. Returns array in [0, 1].\n",
    "    \"\"\"\n",
    "    norm = np.zeros_like(rgb, dtype=np.float32)\n",
    "\n",
    "    for i in range(rgb.shape[0]):\n",
    "        band = rgb[i].astype(np.float32)\n",
    "\n",
    "        # Mask invalid values (0 and NaN)\n",
    "        valid = band[np.isfinite(band) & (band > 0)]\n",
    "        if valid.size == 0:\n",
    "            norm[i] = 0\n",
    "            continue\n",
    "\n",
    "        # Calculating percentiles\n",
    "        p1, p99 = np.percentile(valid, [lower, upper])\n",
    "        if not np.isfinite(p1) or not np.isfinite(p99) or (p99 - p1) < 1e-6:\n",
    "            # Fallback to min-max\n",
    "            vmin, vmax = valid.min(), valid.max()\n",
    "            if vmax - vmin < 1e-6:\n",
    "                norm[i] = 0\n",
    "                continue\n",
    "            p1, p99 = vmin, vmax\n",
    "\n",
    "        norm[i] = np.clip((band - p1) / (p99 - p1), 0, 1)\n",
    "\n",
    "    return norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02928c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We buffer our polygons by 10m. We grab some surroundings by this\n",
    "df_buffered = cleaned_df.copy()\n",
    "df_buffered[\"geometry\"] = df_buffered.buffer(10)\n",
    "\n",
    "# We find all polygons that intersects. So those are potentially the same polygon over some time\n",
    "joined = gpd.sjoin(df_buffered, df_buffered, predicate=\"intersects\", how=\"inner\")\n",
    "\n",
    "# Keep only same-tile matches but different dates\n",
    "joined = joined[joined[\"tile_left\"] == joined[\"tile_right\"]]\n",
    "joined = joined[joined[\"img_date_left\"] != joined[\"img_date_right\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22563d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were found 2582 positive pairs (same location, but different time)\n"
     ]
    }
   ],
   "source": [
    "print(f\"There were found {joined.shape[0]} positive pairs (same location, but different time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee68c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2582/2582 [09:04<00:00,  4.74it/s]\n"
     ]
    }
   ],
   "source": [
    "positive_pairs = []\n",
    "labels = []\n",
    "\n",
    "# Finding, reading and storing all positive pairs (pairs of images with the same polygon but in different times)\n",
    "for _, row in tqdm(joined.iterrows(), total=len(joined)):\n",
    "    try:\n",
    "        minx1, miny1, maxx1, maxy1 = row[[\"minx_left\", \"miny_left\", \"maxx_left\", \"maxy_left\"]]\n",
    "        minx2, miny2, maxx2, maxy2 = row[[\"minx_right\", \"miny_right\", \"maxx_right\", \"maxy_right\"]]\n",
    "        \n",
    "        img1 = load_rgb_crop(\"full_dataset\", row[\"tile_left\"], row[\"img_date_left\"].strftime(\"%Y%m%d\"), minx1, maxx1, miny1, maxy1)\n",
    "        img2 = load_rgb_crop(\"full_dataset\", row[\"tile_right\"], row[\"img_date_right\"].strftime(\"%Y%m%d\"), minx2, maxx2, miny2, maxy2)\n",
    "\n",
    "        if img1 is None or img2 is None:\n",
    "            continue\n",
    "\n",
    "        img1 = robust_percentile_normalize(img1)\n",
    "        img2 = robust_percentile_normalize(img2)\n",
    "\n",
    "        positive_pairs.append((img1, img2))\n",
    "        labels.append(1)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping pair:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb921785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 2300 pairs generated for dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There were {len(positive_pairs)} pairs generated for dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a41ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that all pairs are from different locations\n",
    "df_UXA = cleaned_df[cleaned_df[\"tile\"] == \"36UXA\"]\n",
    "df_UYA = cleaned_df[cleaned_df[\"tile\"] == \"36UYA\"]\n",
    "\n",
    "# Getting some subsets\n",
    "UXA_subset = df_UXA.sample(120, random_state = 50)\n",
    "UYA_subset = df_UYA.sample(120, random_state = 50)\n",
    "\n",
    "# Creating negative pairs (images with different polygons)\n",
    "pairs1 = UXA_subset.iloc[0:40].merge(UYA_subset.iloc[0:40], how=\"cross\", suffixes=(\"_left\", \"_right\"))\n",
    "pairs2 = UXA_subset.iloc[40:80].merge(UYA_subset.iloc[40:80], how=\"cross\", suffixes=(\"_left\", \"_right\"))\n",
    "pairs3 = UXA_subset.iloc[80:120].merge(UYA_subset.iloc[80:120], how=\"cross\", suffixes=(\"_left\", \"_right\"))\n",
    "pairs = pd.concat([pairs1, pairs2, pairs3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74fcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4800/4800 [16:40<00:00,  4.80it/s]\n"
     ]
    }
   ],
   "source": [
    "negative_pairs = []\n",
    "\n",
    "# Finding, reading and storing all negative pairs (images with different polygons)\n",
    "for _, row in tqdm(pairs.iterrows(), total=len(pairs)):\n",
    "    try:\n",
    "        minx1, miny1, maxx1, maxy1 = row[[\"minx_left\", \"miny_left\", \"maxx_left\", \"maxy_left\"]]\n",
    "        minx2, miny2, maxx2, maxy2 = row[[\"minx_right\", \"miny_right\", \"maxx_right\", \"maxy_right\"]]\n",
    "        \n",
    "        img1 = load_rgb_crop(\"full_dataset\", row[\"tile_left\"], row[\"img_date_left\"].strftime(\"%Y%m%d\"), minx1, maxx1, miny1, maxy1)\n",
    "        img2 = load_rgb_crop(\"full_dataset\", row[\"tile_right\"], row[\"img_date_right\"].strftime(\"%Y%m%d\"), minx2, maxx2, miny2, maxy2)\n",
    "\n",
    "        if img1 is None or img2 is None:\n",
    "            continue\n",
    "\n",
    "        img1 = robust_percentile_normalize(img1)\n",
    "        img2 = robust_percentile_normalize(img2)\n",
    "\n",
    "        negative_pairs.append((img1, img2))\n",
    "        labels.append(0)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping pair:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining positive pairs with negative pairs\n",
    "all_pairs = positive_pairs + negative_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23e0b07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5913 pairs for dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(all_pairs)} pairs for dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfabcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train, test and validation parts (80/10/10)\n",
    "pairs_train, pairs_test_val, labels_train, labels_test_val = train_test_split(\n",
    "    all_pairs, labels, test_size=0.2, random_state=50, stratify=labels\n",
    ")\n",
    "\n",
    "pairs_test, pairs_val, labels_test, labels_val = train_test_split(\n",
    "    pairs_test_val, labels_test_val, test_size=0.5, random_state=50, stratify=labels_test_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accbbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving so we don't need to generate dataset again\n",
    "np.savez(\"pairs_train_data.npz\", pairs=pairs_train, labels=labels_train)\n",
    "np.savez(\"pairs_test_data.npz\", pairs=pairs_test, labels=labels_test)\n",
    "np.savez(\"pairs_validation_data.npz\", pairs=pairs_val, labels=labels_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
