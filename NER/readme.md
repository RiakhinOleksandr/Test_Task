#### **Solution explanation**



1. ###### Dataset generation

Dataset generation can be found in dataset\_generation.ipynb file.



Dataset must be balanced, therefore it must have sentences with one mountain name, multiple mountain names and without mountain names.



First step was to get 25000 mountain names from Wikipedia. It was done using SPARQLWrapper library. Query was created, we got the result which was later converted to python list.



To create sentences with one mountain name we created 205 templates of sentences (most of them were generated by ChatGPT). They cover mountains from different sides, so our model could find mountain names in sentences of big range.

Then for each mountain from 25000 mountain names from Wikipedia, 4 random templates were chosen and therefore 4 sentences with each mountain name were created. That is 100000 sentences in our dataset.



Next we have sentences with multiple mountain names. Again 100 templates for sentences with 2 mountain names were created (most of them were generated by ChatGPT). They cover mountains from different sides, so our model could find mountain names in sentences of big range.

Then 8000 random pairs of mountain from those 25000 were chosen. We also made sure that all those pair are not repeatable. For each of those pairs 3 random templates were chosen and therefore 3 sentences with each mountain pair were created. This is 24000 sentences more in our dataset.



Lastly 40 templates for sentences with 3 mountain names were created (most of them were generated by ChatGPT). They cover mountains from different sides, so our model could find mountain names in sentences of big range.

Then 1500 random triples of mountain from those 25000 were chosen. We also made sure that all those triples are not repeatable. For each of those pairs 3 random templates were chosen and therefore 3 sentences with each mountain triple were created. This is 4500 sentences more in our dataset.



Next we must create sentences without mountain names so our dataset will be balanced (otherwise model might think that every sentence has mountain name in it).

We chose 5 different things. Three of them are locations: cities, rivers and islands. Model can confuse some locations with mountain names therefore those locations. Also artists and mathematicians were chosen. This is far away from locations and maybe is not that necessary, but it adds something that is not location in our dataset, which can improve model result in future.



Again we used SPARQLWrapper library and took 3500 cities, 3500 rivers, 3000 islands, 3000 artists and 3000 mathematicians from Wikipedia. The result was converted to python list. Before creating sentence there is always check if this thing has the same name as some mountain from our 25000 mountain names. So not every word was used.



40 templates for sentences with cities were made (most of them were generated by ChatGPT). Some of those templates were pretty close to some mountain templates, so our model can better distinguish mountains from different locations.

Then for each city, 4 random templates were chosen and therefore 4 sentences with each city name were created. That is about 14000 sentences more in our dataset.



40 templates for sentences with rivers were made (most of them were generated by ChatGPT). Some of those templates were pretty close to some mountain templates, so our model can better distinguish mountains from different locations.

Then for each river, 4 random templates were chosen and therefore 4 sentences with each river name were created. That is about 14000 sentences more in our dataset.



30 templates for sentences with islands were made (most of them were generated by ChatGPT).

Then for each island, 4 random templates were chosen and therefore 4 sentences with each island name were created. That is about 12000 sentences more in our dataset.



30 templates for sentences with artists were made (most of them were generated by ChatGPT).

Then for each artist, 4 random templates were chosen and therefore 4 sentences with each artist were created. That is about 12000 sentences more in our dataset.



30 templates for sentences with mathematicians were made (most of them were generated by ChatGPT).

Then for each mathematician, 4 random templates were chosen and therefore 4 sentences with each mathematician were created. That is about 12000 sentences more in our dataset.



Finally we have dataset that contains 192348 rows. 128500 sentences with mountain name or names and the rest without them.



We randomly split dataset into train, test and validation samples (80/10/10) and then saving them in jsonl format because hugging face works good with it.





###### 2\. Selecting the relevant architecture of the model for NER solving

We created dataset, now we need to chose appropriate model to fine-tune it. BERT model were good choice because it gives state-of-the-art performance for NER solving.

DistilBERT-base-cased model was chosen because of it gives great performance and also is much lighter than other BERT model. Maybe BERT-base-cased was better choice, but it is much more computationally dependent. This is one of the problem, about which we will write more in report.





###### 3\. Fine-tuning model

Model training can be found in model\_training.py file.



First of all we need to load our training and validation samples from jsonl files which we made in first chapter.

Then we need to transform our data into more suitable format for working with HuggingFace. So we tokenize all sentences and also creating labels and attaching them to tokens. There are three labels and they have id attaches to them: 0 - "O" (not mountain), 1 - "B-MOUNTAIN" (begining of mountain name), 2 - "I-MOUNTAIN" (inside of mountain name).



After that we start to prepare our model. We setting number of labels and bijection between labels and id.



Then we setting training arguments. We set directory where output will be putted.

We set learning rate as 3e-5. It is common practice to set small learning rate (1e-5 - 5e-5) for fine-tuning model, so model slightly adjusts the weights to adapt to finding mountains. We don't want to completely rewrite what the model already knows.

We set number of epochs to 3. 3 is good choice because it is not too big, so model is not overfitting, but also not too small, so it learns from our dataset.

We set weight decay. It helps model keep weight not too big, helping generalization and preventing overfitting.

We are using GPU to train our model. It makes training much faster.

We are logging information every 200 steps. It helps to trace development of our model, we see how loss function is decreasing. If it decreases too fast, maybe we are overfitting model. If it decreasing too slow, it is also a problem.

We also choose save strategy and evaluation strategy. So after every epoch is ended, we save checkpoint and evaluate result on our validation dataset.



After that we start training, passing train and validation dataset. It took about an hour to train on model on GPU.



Lastly, we save model and tokenizer, so we don't need to train every time.





###### 4\. Testing model

Model testing can be found in model\_testing.py file.



First of all we load test dataset from our jsonl file. We also load model that we trained in previous chapter and tokenizer.



Then we creating pipeline to work with our model. We also are using GPU for faster predictions, when multiple sentences are given.



Then we pass sentences from our test dataset to model, getting predictions. We compare predictions with real result and using classification\_report function printing some important metrics like precision, recall and F-score.



We also can look closer to where our model made mistake. We count number of mistakes and if there no more than 15 mistakes, we can look on all of them. If there are more, we are looking at 15 random chosen. Our model made mistake on only one sentence, which is a great result.





###### 5\. Preparing demo

Demo starts with explanation of what it does and what it contains.



Then it lists all libraries that are needed to run demo and command that will install them if they aren't installed already.



Then after importing all necessary libraries. We are loading our model and tokenizer.



We start with some basic examples of how model works. We created some sentences with mountains and without mountains and passed them to our model. The result is printed out.



After that we load test dataset and again as in model\_testing.py file, compare predictions with real result and using classification\_report function printing some important metrics like precision, recall and F-score.



There is also cell, so you can test model right in demo.







#### Details on how to set up project.

If you want just to use this model you only need to download demo, mountain-ner-distilbert and mountain-ner-model folders. In demo there is example how to use model and even interactive cell for testing model.



If you want to repeat process of creating that model, you need to follow instructions:

1. First of all download all .py and .ipynb files.
2. Assuming you already have Python 3.12 (On moment of writing this readme file, torch doesn't support using GPU for faster training and predictions on Python 3.13, so I needed to downgrade to Python 3.12) you need to download all libraries listed in requirements.txt. You can do by writing next command in console: pip install 'library\_name'.
3. To use GPU for faster training and predictions your GPU must support CUDA (If you have NVIDIA GPU you need to check version of CUDA it supports. Then you can import torch and by using torch.cuda.is\_available() function check. If it is False you might need to run next command (replace 121 with your version of CUDA if necessary): 'pip install torch==2.2.2+cu121 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121').
4. If you have not NVIDIA GPU or torch.cuda.is\_available() says False no matter what you do, it is okay. It will run by CPU, it is much slower, but it must work. If torch.cuda.is\_available() says True, you can go to next step.
5. Open dataset\_generation.ipynb and run all cells. At the end three jsonl files (train.jsonl, test.jsonl, validation.jsonl) must appear in your directory. In cell where number of rows is printed you must have something about 192500 (192348 in my case. It might change if something will change in Wikipedia).
6. Next step is to open model\_training.py file and run it. If everything okay, you will see progress bar and see training results printed every 200 steps. For me training model took a little bit more than an hour. After training will end, you must see three new directories in yours (mountain-ner, mountain-ner-distilbert and mountain-ner-model). mountain-ner directory contains checkoints, others two diretories are more important, they contain model and tokenizer.
7. Now after you have trained you model, you can run model\_testing.py or demo.ipynb to check result you've got.
